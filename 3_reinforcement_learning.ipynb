{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589389e6-2aad-45b6-bc03-e9b37ab1514f",
   "metadata": {},
   "source": [
    "# Reinforcement learning for legged robots\n",
    "\n",
    "In this notebook, \n",
    "\n",
    "## Setup\n",
    "\n",
    "Before we start, you will need to update your conda environment to use Gymnasium (maintained) rather than OpenAI Gym (discontinued). You can simply run:\n",
    "\n",
    "```\n",
    "conda activate robotics-mva\n",
    "conda install gymnasium imageio mujoco=2.3.7 tensorboard\n",
    "```\n",
    "\n",
    "Import Gymnasium to check that everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa7b15-843f-4696-9bfc-141da71bf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e264559-88cc-48b7-8257-f7755fff3ce7",
   "metadata": {},
   "source": [
    "Let's import the usual suspects as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86303cf2-f879-407d-b528-6c0a80b8df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e16a1-bf4a-403b-8709-3da11bc3c4b4",
   "metadata": {},
   "source": [
    "# Inverted pendulum environment\n",
    "\n",
    "The inverted pendulum model is not just a toy model reproducing the properties of real robot models for balancing: as it turns out, the inverted pendulum appears in the dynamics of *any* mobile robot, that is, a model with a floating-base joint at the root of the kinematic tree. (If you are curious: the inverted pendulum is a limit case of the [Newton-Euler equations](https://scaron.info/robotics/newton-euler-equations.html) corresponding to floating-base coordinates in the equations of motion $M \\ddot{q} + h = S^T \\tau + J_c^T f$, in the limit where the robot [does not vary its angular momentum](https://scaron.info/robotics/point-mass-model.html).) Thus, while we work on a simplified inverted pendulum in this notebook, concepts and tools are those used as-is on real robots, as you can verify by exploring the bonus section.\n",
    "\n",
    "Gymnasium is mainly a single-agent reinforcement learning API, but it also comes with simple environments, including an inverted pendulum sliding on a linear guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5d942-85fa-435e-b5ef-8c85a74ba3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gym.make(\"InvertedPendulum-v4\", render_mode=\"human\") as env:\n",
    "    action = 0.0 * env.action_space.sample()\n",
    "    observation, _ = env.reset()\n",
    "    episode_return = 0.0\n",
    "    for step in range(200):\n",
    "        # action[0] = 5.0 * observation[1] + 0.3 * observation[0]\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_return += reward\n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset()\n",
    "            episode_return = 0.0\n",
    "    print(f\"Return of the episode: {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7322422-94db-4e12-b299-36bb40649cf7",
   "metadata": {},
   "source": [
    "The structure of the action and observation vectors are documented in [Inverted Pendulum - Gymnasium Documentation](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/).  The observation, in particular, is a NumPy array with four coordinates that we recall here for reference:\n",
    "\n",
    "| Num | Observation | Min | Max | Unit |\n",
    "|-----|-------------|-----|-----|------|\n",
    "|   0 | position of the cart along the linear surface | -Inf | Inf | position (m) |\n",
    "|   1 | vertical angle of the pole on the cart | -Inf | Inf | angle (rad) |\n",
    "|   2 | linear velocity of the cart | -Inf | Inf | linear velocity (m/s) |\n",
    "|   3 | angular velocity of the pole on the cart | -Inf | Inf | anglular velocity (rad/s) |\n",
    "\n",
    "Check out the documentation for the definitions of the action and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285d7ce-3a97-4b07-8b5f-a9b04d7721ab",
   "metadata": {},
   "source": [
    "## PID balancer\n",
    "\n",
    "A *massively* used class of policies is the [PID controller](https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller). Let's say we have a reference observation, like $o^* = [0\\ 0\\ 0\\ 0]$ for the inverted pendulum. Denoting by $e = o^* - o$ the *error* of the system when it observes a given state, a PID controller will apply the action:\n",
    "\n",
    "$$\n",
    "a(t) = K_p^T e(t) + K_d^T \\dot{e}(t) + K_i^T \\int e(\\tau) \\mathrm{d} \\tau\n",
    "$$\n",
    "\n",
    "where $K_{p}, K_i, K_d \\in \\mathbb{R}^4$ are constants called *gains* and tuned by the user. In discrete time:\n",
    "\n",
    "$$\n",
    "a_k = K_p^T e_k + K_d^T \\frac{e_k - e_{k-1}}{\\delta t} + K_i^T \\sum_{i=0}^{k} e_i {\\delta t}\n",
    "$$\n",
    "\n",
    "Let's refactor the rolling out of our episode into a standalone function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c839bc6-168a-42c3-8f1c-c6b0c5411901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(policy, show: bool = True):\n",
    "    episode = []\n",
    "    kwargs = {\"render_mode\": \"human\"} if show else {}\n",
    "    with gym.make(\"InvertedPendulum-v4\", **kwargs) as env:\n",
    "        observation, _ = env.reset()\n",
    "        episode.append(observation)\n",
    "        for step in range(1000):\n",
    "            action = policy(observation)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.extend([action, reward, observation])\n",
    "            if terminated or truncated:\n",
    "                return episode\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff0dce-a4df-4917-bb17-2393353610a3",
   "metadata": {},
   "source": [
    "## Question 1: Write a PID controller that balances the inverted pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfb28b-ff73-42ff-9524-eac8ec12f8a1",
   "metadata": {},
   "source": [
    "You can use global variables to store the (discrete) derivative and integral terms, this will be OK here as we only rollout a single trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ddcef-c0f7-4251-b73f-d5df5a0027e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_observation = np.zeros(4)\n",
    "integral = np.zeros(4)\n",
    "\n",
    "def pid_control(observation: np.ndarray) -> np.ndarray:\n",
    "    global previous_observation, integral\n",
    "    derivative = observation - previous_observation\n",
    "    previous_observation = observation.copy()\n",
    "    integral += observation\n",
    "    my_action_value: float = 5.0 * observation[1] + 0.15 * observation[0] - 0.01 * observation[2]\n",
    "    return np.array([my_action_value])\n",
    "\n",
    "episode = rollout(pid_control, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a005aa-87fa-4f98-8ace-f24421886bed",
   "metadata": {},
   "source": [
    "You can look at the system using `show=True`, but intuition usually builds faster when looking at relevant plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5decd-779c-4f0d-84fd-3eb47358b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.array(episode[::3])\n",
    "\n",
    "plt.plot(observations)\n",
    "plt.legend((\"pitch\", \"position\", \"linear_velocity\", \"angular_velocity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d50cd2-26fa-4d3c-a671-1ed0e1b9ee93",
   "metadata": {},
   "source": [
    "Can you reach the full reward of 1000 steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacbd0a-2ac5-44cf-848b-8ebfb6fe35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Return of the episode: {sum(episode[2::3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dd0df-6dc7-4d51-b29b-c77b49bde437",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bonus: train for a real robot\n",
    "\n",
    "This section is entirely optional and will only work on Linux or macOS. In this part, we follow the same training pipeline but with the open source software of [Upkie](https://hackaday.io/project/185729-upkie-wheeled-biped-robots)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634ff93-f09f-4e0a-8d0f-547848f3900b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1189580/170496331-e1293dd3-b50c-40ee-9c2e-f75f3096ebd8.png\" style=\"height: 100px\" align=\"right\" />\n",
    "\n",
    "First, make sure you have a C++ compiler (setup one-liners: [Fedora](https://github.com/upkie/upkie/discussions/100), [Ubuntu](https://github.com/upkie/upkie/discussions/101)). You can run an Upkie simulation right from the command line. It won't install anything on your machine, everything will run locally from the repository:\n",
    "\n",
    "```console\n",
    "git clone https://github.com/upkie/upkie.git\n",
    "cd upkie\n",
    "./start_simulation.sh\n",
    "```\n",
    "\n",
    "We will use the Python API of the robot to test things from this notebook, or from custom scripts. Install it from PyPI in your Conda environment:\n",
    "\n",
    "```\n",
    "pip install upkie\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44abc0-f7e9-4c2b-9d4e-a3579213e138",
   "metadata": {},
   "source": [
    "## Stepping the environment\n",
    "\n",
    "If everything worked well, you should be able to step an environment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedf0d6-fc2f-43f4-9ff6-a8e12dbd7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gymW\n",
    "import upkie.envs\n",
    "\n",
    "upkie.envs.register()\n",
    "\n",
    "episode_return = 0.0\n",
    "with gym.make(\"UpkieGroundVelocity-v1\", frequency=200.0) as env:\n",
    "    observation, _ = env.reset()  # connects to the spine (simulator or real robot)\n",
    "    action = 0.0 * env.action_space.sample()\n",
    "    for step in range(1000):\n",
    "        pitch = observation[0]\n",
    "        action[0] = 10.0 * pitch  # 1D action: [ground_velocity]\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_return += reward\n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset()\n",
    "\n",
    "print(f\"We have stepped the environment {step + 1} times\")\n",
    "print(f\"The return of our episode is {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031343b5-cf94-46ae-98f3-a4c5ebbc037c",
   "metadata": {},
   "source": [
    "(If you see a message \"Waiting for spine /vulp to start\", it means the simulation is not running.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfd91f-676c-4d6f-beb0-a286dc681ae3",
   "metadata": {},
   "source": [
    "We can double-check the last observation from the episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d972f-4cc9-4005-b9a1-4a7433a19938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_last_observation(observation):\n",
    "    print(\"The last observation of the episode is:\")\n",
    "    print(f\"- Pitch from torso to world: {observation[0]:.2} rad\")\n",
    "    print(f\"- Ground position: {observation[1]:.2} m\")\n",
    "    print(f\"- Angular velocity from torso to world in torso: {observation[2]:.2} rad/s\")\n",
    "    print(f\"- Ground velocity: {observation[3]:.2} m/s\")\n",
    "    \n",
    "report_last_observation(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a269e3-d876-4d05-88e5-b0d73be6f939",
   "metadata": {},
   "source": [
    "## Question B1: PID balancer\n",
    "\n",
    "Adapt your code from Question 1 to this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e9f1e-f2a1-4a18-aa38-256a425d018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_b1(observation):\n",
    "    return np.array([0.0])  # replace with your solution\n",
    "\n",
    "\n",
    "def run(policy, nb_steps: int):\n",
    "    episode_return = 0.0\n",
    "    with gym.make(\"UpkieGroundVelocity-v1\", frequency=200.0) as env:\n",
    "        observation, _ = env.reset()  # connects to the spine (simulator or real robot)\n",
    "        for step in range(nb_steps):\n",
    "            action = policy_b1(observation)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                print(\"Fall detected!\")\n",
    "                return episode_return\n",
    "    report_last_observation(observation)\n",
    "    return episode_return\n",
    "\n",
    "\n",
    "episode_return = run(policy_b1, 1000)\n",
    "print(f\"The return of our episode is {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999eb22-a94a-4a58-ac06-9a7dbc15a7ee",
   "metadata": {},
   "source": [
    "## Training a new policy\n",
    "\n",
    "The Upkie repository ships three agents based on PID control, model predictive control and reinforcement learning. We now focus on the latter, called the \"PPO balancer\".\n",
    "\n",
    "Check that you can run the training part by running, from the root of the repository:\n",
    "\n",
    "```\n",
    "./tools/bazel run //agents/ppo_balancer:train -- --nb-envs 1 --show\n",
    "```\n",
    "\n",
    "A simulation window should pop, and verbose output from SB3 should be printed to your terminal.\n",
    "\n",
    "By default, training data will be logged to `/tmp`. You can select a different output path by setting the `UPKIE_TRAINING_PATH` environment variable in your shell. For instance:\n",
    "\n",
    "```\n",
    "export UPKIE_TRAINING_PATH=\"${HOME}/src/upkie/training\"\n",
    "```\n",
    "\n",
    "Run TensorBoard from the training directory:\n",
    "\n",
    "```\n",
    "tensorboard --logdir ${UPKIE_TRAINING_PATH}  # or /tmp if you keep the default\n",
    "```\n",
    "\n",
    "### Selecting the number of processes\n",
    "\n",
    "We can increase the number of parallel CPU environments ``--nb-envs`` to a value suitable to your computer. Let training run for a minute and check `time/fps`. Increase the number of environments and compare the stationary regime of `time/fps`. You should see a performance increase when adding the first few environments, followed by a declined when there are two many parallel processes compared to your number of CPU cores. Pick the value that works best for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6d7e4-fec4-479f-8011-d044b4693a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
