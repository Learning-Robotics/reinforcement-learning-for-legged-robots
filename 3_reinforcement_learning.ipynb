{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589389e6-2aad-45b6-bc03-e9b37ab1514f",
   "metadata": {},
   "source": [
    "# Reinforcement learning for legged robots\n",
    "\n",
    "In this notebook, \n",
    "\n",
    "## Setup\n",
    "\n",
    "Before we start, you will need to update your conda environment to use Gymnasium (maintained) rather than OpenAI Gym (discontinued). You can simply run:\n",
    "\n",
    "```\n",
    "conda activate robotics-mva\n",
    "conda install gymnasium imageio mujoco=2.3.7 stable-baselines3 tensorboard\n",
    "```\n",
    "\n",
    "Import Gymnasium and Stable Baselines3 to check that everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fa7b15-843f-4696-9bfc-141da71bf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e264559-88cc-48b7-8257-f7755fff3ce7",
   "metadata": {},
   "source": [
    "Let's import the usual suspects as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86303cf2-f879-407d-b528-6c0a80b8df20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7ff5acb1fe80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e16a1-bf4a-403b-8709-3da11bc3c4b4",
   "metadata": {},
   "source": [
    "# Inverted pendulum environment\n",
    "\n",
    "The inverted pendulum model is not just a toy model reproducing the properties of real robot models for balancing: as it turns out, the inverted pendulum appears in the dynamics of *any* mobile robot, that is, a model with a floating-base joint at the root of the kinematic tree. (If you are curious: the inverted pendulum is a limit case of the [Newton-Euler equations](https://scaron.info/robotics/newton-euler-equations.html) corresponding to floating-base coordinates in the equations of motion $M \\ddot{q} + h = S^T \\tau + J_c^T f$, in the limit where the robot [does not vary its angular momentum](https://scaron.info/robotics/point-mass-model.html).) Thus, while we work on a simplified inverted pendulum in this notebook, concepts and tools are those used as-is on real robots, as you can verify by exploring the bonus section.\n",
    "\n",
    "Gymnasium is mainly a single-agent reinforcement learning API, but it also comes with simple environments, including an inverted pendulum sliding on a linear guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b5d942-85fa-435e-b5ef-8c85a74ba3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return of the episode: 200.0\n"
     ]
    }
   ],
   "source": [
    "with gym.make(\"InvertedPendulum-v4\", render_mode=\"human\") as env:\n",
    "    action = 0.0 * env.action_space.sample()\n",
    "    observation, _ = env.reset()\n",
    "    episode_return = 0.0\n",
    "    for step in range(200):\n",
    "        # action[0] = 5.0 * observation[1] + 0.3 * observation[0]\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_return += reward\n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset()\n",
    "    print(f\"Return of the episode: {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7322422-94db-4e12-b299-36bb40649cf7",
   "metadata": {},
   "source": [
    "The structure of the action and observation vectors are documented in [Inverted Pendulum - Gymnasium Documentation](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/).  The observation, in particular, is a NumPy array with four coordinates that we recall here for reference:\n",
    "\n",
    "| Num | Observation | Min | Max | Unit |\n",
    "|-----|-------------|-----|-----|------|\n",
    "|   0 | position of the cart along the linear surface | -Inf | Inf | position (m) |\n",
    "|   1 | vertical angle of the pole on the cart | -Inf | Inf | angle (rad) |\n",
    "|   2 | linear velocity of the cart | -Inf | Inf | linear velocity (m/s) |\n",
    "|   3 | angular velocity of the pole on the cart | -Inf | Inf | anglular velocity (rad/s) |\n",
    "\n",
    "We will use the following labels to annotate plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a3231c70-f49d-49be-b260-aadbade7b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_LEGEND = (\"pitch\", \"position\", \"linear_velocity\", \"angular_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa062536-204c-4312-a858-f992f3db61d6",
   "metadata": {},
   "source": [
    "Check out the documentation for the definitions of the action and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285d7ce-3a97-4b07-8b5f-a9b04d7721ab",
   "metadata": {},
   "source": [
    "# PID control\n",
    "\n",
    "A *massively* used class of policies is the [PID controller](https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller). Let's say we have a reference observation, like $o^* = [0\\ 0\\ 0\\ 0]$ for the inverted pendulum. Denoting by $e(t) = o^* - o(t)$ the *error* of the system when it observes a given state, a continuous-time PID controller will apply the action:\n",
    "\n",
    "$$\n",
    "a(t) = K_p^T e(t) + K_d^T \\dot{e}(t) + K_i^T \\int e(\\tau) \\mathrm{d} \\tau\n",
    "$$\n",
    "\n",
    "where $K_{p}, K_i, K_d \\in \\mathbb{R}^4$ are constants called *gains* and tuned by the user. In discrete time the idea is the same:\n",
    "\n",
    "$$\n",
    "a_k = K_p^T e_k + K_d^T \\frac{e_k - e_{k-1}}{\\delta t} + K_i^T \\sum_{i=0}^{k} e_i {\\delta t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c381eb-fca9-4ef4-8f99-3b1943231654",
   "metadata": {},
   "source": [
    "Let's refactor the rolling out of our episode into a standalone function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c839bc6-168a-42c3-8f1c-c6b0c5411901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_from_env(env, policy):\n",
    "    episode = []\n",
    "    observation, _ = env.reset()\n",
    "    episode.append(observation)\n",
    "    for step in range(1000):\n",
    "        action = policy(observation)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.extend([action, reward, observation])\n",
    "        if terminated or truncated:\n",
    "            return episode\n",
    "    return episode\n",
    "\n",
    "def rollout(policy, show: bool = True):\n",
    "    kwargs = {\"render_mode\": \"human\"} if show else {}\n",
    "    with gym.make(\"InvertedPendulum-v4\", **kwargs) as env:\n",
    "        episode = rollout_from_env(env, policy)\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff0dce-a4df-4917-bb17-2393353610a3",
   "metadata": {},
   "source": [
    "## Question 1: Write a PID controller that balances the inverted pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfb28b-ff73-42ff-9524-eac8ec12f8a1",
   "metadata": {},
   "source": [
    "You can use global variables to store the (discrete) derivative and integral terms, this will be OK here as we only rollout a single trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045ddcef-c0f7-4251-b73f-d5df5a0027e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_observation = np.zeros(4)\n",
    "integral = np.zeros(4)\n",
    "\n",
    "def pid_policy(observation: np.ndarray) -> np.ndarray:\n",
    "    global previous_observation, integral\n",
    "    derivative = observation - previous_observation\n",
    "    previous_observation = observation.copy()\n",
    "    integral += observation\n",
    "    my_action_value: float = 5.0 * observation[1] + 0.15 * observation[0] - 0.01 * observation[2]\n",
    "    return np.array([my_action_value])\n",
    "\n",
    "episode = rollout(pid_policy, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a005aa-87fa-4f98-8ace-f24421886bed",
   "metadata": {},
   "source": [
    "You can look at the system using `show=True`, but intuition usually builds faster when looking at relevant plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5decd-779c-4f0d-84fd-3eb47358b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.array(episode[::3])\n",
    "\n",
    "plt.plot(observations)\n",
    "plt.legend(OBSERVATION_LEGEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d50cd2-26fa-4d3c-a671-1ed0e1b9ee93",
   "metadata": {},
   "source": [
    "Can you reach the full reward of 1000 steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bacbd0a-2ac5-44cf-848b-8ebfb6fe35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return of the episode: 1000.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Return of the episode: {sum(episode[2::3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cc998-1b23-416f-8e3b-810100c223fb",
   "metadata": {},
   "source": [
    "# Policy optimization\n",
    "\n",
    "Let us now train a policy, parameterized by a multilayer perceptron (MLP), to maximize the expected return over episodes on the inverted pendulum environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5631f0f-1b84-4ee6-8e9c-b4f2915bd281",
   "metadata": {},
   "source": [
    "## Our very first policy\n",
    "\n",
    "We will use the proximal policy optimization (PPO) algorithm for training, using the implementation from Stable Baselines3: [PPO - Stable Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128867ca-e600-4ba1-abbd-1f918976fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "with gym.make(\"InvertedPendulum-v4\", render_mode=\"human\") as env:\n",
    "    first_policy = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    first_policy.learn(total_timesteps=1000, progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323400b-18ca-43f6-a81e-a5e7f033a536",
   "metadata": {},
   "source": [
    "By instantiating the algorithm with no further ado, we let the library decide for us on a sane set of default hyperparameters, including:\n",
    "\n",
    "- Rollout buffers of `n_steps = 2048` steps, which we will visit `n_epochs = 10` times with mini-batches of size `batch_size = 64`.\n",
    "- Clipping range: ``0.2``.\n",
    "- No entropy regularization.\n",
    "- Learning rate for the Adam optimizer: ``3e-4``\n",
    "- Policy and value-function network architectures: two layers of 64 neurons with $\\tanh$ activation functions.\n",
    "\n",
    "We then called the `learn` function to execute PPO over a fixed total number of timesteps, here just a thousand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82173c-6609-4b83-8618-36f82c1c1373",
   "metadata": {},
   "source": [
    "Rendering actually took a significant chunk of time. Let's instantiate and keep an environment open without rendering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "460fe1c7-ee3b-450a-b09c-03b96f9086bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedPendulum-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd090f-ca34-41e0-9900-52977eef9c4b",
   "metadata": {},
   "source": [
    "We can use it to train much more steps in roughly the same time, reporting training metrics every `n_steps` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7262602-c277-4697-8987-ba126a87e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.89     |\n",
      "|    ep_rew_mean     | 8.89     |\n",
      "| time/              |          |\n",
      "|    fps             | 1727     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.7       |\n",
      "|    ep_rew_mean          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1260       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03086083 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | -0.00642   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.34       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.943      |\n",
      "|    value_loss           | 17.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.3        |\n",
      "|    ep_rew_mean          | 16.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1108        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027575245 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.57        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.902       |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.3        |\n",
      "|    ep_rew_mean          | 27.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015638145 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.863       |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.9        |\n",
      "|    ep_rew_mean          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1007        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012239402 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.0899      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 53          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7ff5a4346fe0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_policy = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "second_policy.learn(total_timesteps=10_000, progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219aab8-1143-4606-a44f-b62fdffebbf1",
   "metadata": {},
   "source": [
    "Let's see how this policy performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17dc178-bb9c-4155-8047-feed1e575226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_closure(policy):\n",
    "    \"\"\"Utility function to turn our policy instance into a function.\n",
    "\n",
    "    Args:\n",
    "        policy: Policy to turn into a function.\n",
    "        \n",
    "    Returns:\n",
    "        Function from observation to policy action.\n",
    "    \"\"\"\n",
    "    def policy_function(observation):\n",
    "        action, _ = policy.predict(observation)\n",
    "        return action\n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4e3cd4-4572-4c40-94b1-688d472a4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frankd/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/glfw/__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n",
      "Exception ignored in: <function WindowViewer.__del__ at 0x7ff5abe4fac0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frankd/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/home/frankd/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/home/frankd/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/glfw/__init__.py\", line 1279, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    }
   ],
   "source": [
    "episode = rollout(policy_closure(second_policy), show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941219f1-9b3c-4e66-86e5-d42f2473b149",
   "metadata": {},
   "source": [
    "Okay, it looks like we didn't train for long enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f2a85-8dd7-4f3a-8368-7427f1caadca",
   "metadata": {},
   "source": [
    "## Monitoring performance during training\n",
    "\n",
    "Let's train for longer, and use TensorBoard to keep track. We don't know how long training will take so let's put a rather large total number of steps (you can interrupt training once you observed convergence in TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196da0ad-1e83-441e-ac10-6c9ecd83c224",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m erudite_policy \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./inverted_pendulum_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43merudite_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 272\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    274\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "erudite_policy = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    tensorboard_log=\"./inverted_pendulum_tensorboard/\",\n",
    "    verbose=0,\n",
    ")\n",
    "erudite_policy.learn(\n",
    "    total_timesteps=1_000_000,\n",
    "    progress_bar=False,\n",
    "    tb_log_name=\"erudite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91d14e-53e2-443f-b7ab-edd69d480add",
   "metadata": {},
   "source": [
    "Run TensorBoard on the directory thus created to open a dashboard in your Web browser:\n",
    "\n",
    "```\n",
    "tensorboard --logdir ./inverted_pendulum_tensorboard/\n",
    "```\n",
    "\n",
    "The link will typically be http://localhost:6006 (port number increases if you run TensorBoard multiple times in parallel). Tips:\n",
    "\n",
    "- Click the Settings icon in the top-right corner and enable \"Reload data\"\n",
    "- Uncheck \"Ignore outliers in chart scaling\" (your preference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68771e35-48cd-43be-89ff-0055dc196d0b",
   "metadata": {},
   "source": [
    "## Saving our policy\n",
    "\n",
    "Now that you spent some computing to optimize an actual policy, better save it to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "effeccfc-8b95-48e1-98c4-1b96838bb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "erudite_policy.save(\"pendulum_erudite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09c56e-8647-414d-aae0-5e1b16ba3a0f",
   "metadata": {},
   "source": [
    "You can then reload it later by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994dae4f-b651-4488-925e-2ba369eeedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "erudite_policy = PPO.load(\"pendulum_erudite\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded4f1e-ae57-4c94-b2e1-3a6d019aecc4",
   "metadata": {},
   "source": [
    "## Question 2: How many steps does it take to train a successful policy?\n",
    "\n",
    "We consider a policy successful if it consistently achieves the maximum return of 1000."
   ]
  },
  {
   "cell_type": "raw",
   "id": "42c6d68d-4812-4222-97da-a6699803b986",
   "metadata": {},
   "source": [
    "== Your reply here =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b846f-db13-43ba-81cb-57b039852c86",
   "metadata": {},
   "source": [
    "## A more realistic environment\n",
    "\n",
    "Real systems suffer from the two main issues we saw in the [Perception and estimation](https://scaron.info/robotics-mva/#5-perception-estimation) class: *bias* and *variance*. In this section, we model bias in actuation and perception by adding delays (via low-pass filtering) to respectively the action and observation vectors. Empirically this is an effective model, as for instance it contributes to sim2real transfer on Upkie. To add these delays, we use an [`environment wrapper`](https://gymnasium.farama.org/api/wrappers/), which is a convenient way to compose environments, used in both the Gymnasium and Stable Baselines3 APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6e8a3140-7ee7-4d6f-afd9-19d6ca4816c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelayWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, time_constant: float = 0.2):\n",
    "        \"\"\"Wrap environment with some actuation and perception modeling.\n",
    "\n",
    "        Args:\n",
    "            env: Environment to wrap.\n",
    "            time_constant: Constant of the internal low-pass filter, in seconds.\n",
    "                Feel free to play with different values but leave it to the default\n",
    "                of 0.2 seconds when handing out your homework.\n",
    "\n",
    "        Note:\n",
    "            Delays are implemented by a low-pass filter. The same time constant\n",
    "            is used for both actions and observations, which is not realistic, but\n",
    "            makes for less tutorial code ;)\n",
    "        \"\"\"\n",
    "        alpha = env.dt / time_constant\n",
    "        assert 0.0 < alpha < 1.0\n",
    "        super().__init__(env)\n",
    "        self._alpha = alpha\n",
    "        self._prev_action = 0.0 * env.action_space.sample()\n",
    "        self._prev_observation = np.zeros(4)\n",
    "\n",
    "    def low_pass_filter(self, old_value, new_value):\n",
    "        return old_value + self._alpha * (new_value - old_value)\n",
    "        \n",
    "    def step(self, action):\n",
    "        new_action = self.low_pass_filter(self._prev_action, action)\n",
    "        observation, reward, terminated, truncated, info = self.env.step(new_action)\n",
    "        new_observation = self.low_pass_filter(self._prev_observation, observation)\n",
    "        self._prev_action = new_action\n",
    "        self._prev_observation = new_observation\n",
    "        return new_observation, reward, terminated, truncated, info\n",
    "\n",
    "delay_env = DelayWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5de5e-50ca-4049-bb5f-b9203919e0ba",
   "metadata": {},
   "source": [
    "We can check how our current policy fares against the delayed environment. Spoiler alert: no great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4e1508e6-e04f-4b22-8009-80baae1bae7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff52b3589d0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "delay_episode = rollout_from_env(delay_env, policy_closure(erudite_policy))\n",
    "\n",
    "plt.plot(np.array(delay_episode[::3])[:, :2])\n",
    "plt.legend(OBSERVATION_LEGEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af3932-751e-47e4-8334-bd55be62aaa1",
   "metadata": {},
   "source": [
    "## Question 3: Can't we just re-train a policy on the new environment?\n",
    "\n",
    "At this point of the tutorial this is a rethorical question, but we should check anyway. Re-train a policy on the delayed environment and comment on its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "693aa97c-3ee2-4cbd-bc06-7cb224e8bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48928906-bcd9-40d5-b17e-35fd06d6c6ac",
   "metadata": {},
   "source": [
    "== Your observations here =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "49815985-4d82-401d-a980-7ac6e1fbbf49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m delay_policy \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, delay_env, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./inverted_pendulum_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdelay_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:210\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 210\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/policies.py:694\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    692\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 694\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:222\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/torch/nn/modules/activation.py:356\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delay_policy = PPO(\"MlpPolicy\", delay_env, tensorboard_log=\"./inverted_pendulum_tensorboard/\", verbose=0)\n",
    "delay_policy.learn(total_timesteps=100_000, progress_bar=False, tb_log_name=\"delay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2df30-259f-477a-ab14-d39c17e5f15f",
   "metadata": {},
   "source": [
    "## The Real Question 3: Why do delays degrade both runtime and training performance?\n",
    "\n",
    "Loss in runtime performance refers to the one we observed when executing a policy trained without delay on an environment with delays. Loss in training performance refers to the fact that, even when we train a new policy on the environment with delays, by the end of training it does not achieve maximum return."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b7459d5-93d0-49cb-85c7-2172e2b08073",
   "metadata": {},
   "source": [
    "== Your explanation here =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a441a-a84d-49ab-aecc-7362dee66b91",
   "metadata": {},
   "source": [
    "Propose and implement a way to overcome this. Train the resulting policy in a variable called `iron_policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b22770ba-4e58-4989-b62c-d5aa1734336c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m stacked_env \u001b[38;5;241m=\u001b[39m FrameStack(delay_env, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m iron_policy \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacked_env, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./inverted_pendulum_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43miron_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/robotics-mva/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:175\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m clipped_actions \u001b[38;5;241m=\u001b[39m actions\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m    178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(clipped_actions)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "stacked_env = FrameStack(delay_env, 10)\n",
    "iron_policy = PPO(\"MlpPolicy\", stacked_env, tensorboard_log=\"./inverted_pendulum_tensorboard/\", verbose=0)\n",
    "iron_policy.learn(total_timesteps=100_000, progress_bar=False, tb_log_name=\"iron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a876f-e78e-47bb-9b42-9423618d1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "iron_policy.save(\"iron_policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a70b63-7fda-4c0f-b777-ef0dc2128ab2",
   "metadata": {},
   "source": [
    "Roll out an episode and plot the outcome to show that your policy handles delays properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f13a2b-6a1c-4d44-bb84-2fffaf6bf6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = rollout_from_env(stacked_env, policy_closure(iron_policy))\n",
    "observations = [x[-1] for x in episode[::3]]\n",
    "plt.plot(np.array(observations)[:, :2])\n",
    "plt.legend(OBSERVATION_LEGEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12fcf1-88b9-4899-b79d-866c67e4a3f5",
   "metadata": {},
   "source": [
    "## Question 4: Can you improve sampling efficiency?\n",
    "\n",
    "This last question is open: what can you change in the pipeline to train a policy that achieves maximum return using less environment steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d78dd-f80e-4183-8fa7-55c803e38404",
   "metadata": {},
   "source": [
    "Here is a wrapper template if you want to experiment with reward shaping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9a3ed-8f76-4fac-98f2-3a23df818deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "   def __init__(self, env):\n",
    "       super().__init__(env)\n",
    "\n",
    "   def step(self, action):\n",
    "       observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "       new_reward = 0.0  # your formula here\n",
    "       return observation, new_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1ae1d-82be-416a-b439-acb5f9a545c3",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cede7-e5a2-4154-9810-877d51c115c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_schedule(y_0: float, y_1: float):\n",
    "    \"\"\"Affine schedule as a function over the [0, 1] interval.\n",
    "\n",
    "    Args:\n",
    "        y_0: Function value at zero.\n",
    "        y_1: Function value at one.\n",
    "        \n",
    "    Returns:\n",
    "        Corresponding affine function.\n",
    "    \"\"\"\n",
    "    def schedule(x: float) -> float:\n",
    "        return y_0 + x * y_1 - y_0\n",
    "    return schedule\n",
    "\n",
    "\n",
    "rewarding_env = FrameStack(DelayWrapper(CustomRewardWrapper(env)), 10)\n",
    "final_policy = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    rewarding_env,\n",
    "    tensorboard_log=\"./inverted_pendulum_tensorboard/\",\n",
    "    verbose=0,\n",
    "    learning_rate=affine_schedule(\n",
    "        y_1=3e-3,  # progress_remaining=1.0\n",
    "        y_0=3e-4,  # progress_remaining=0.0\n",
    "    ),\n",
    "    clip_range=0.1,\n",
    ")\n",
    "final_policy.learn(\n",
    "    total_timesteps=200_000,\n",
    "    tb_log_name=\"final_policy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dd0df-6dc7-4d51-b29b-c77b49bde437",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bonus: training a policy for a real robot\n",
    "\n",
    "This section is entirely optional and will only work on Linux or macOS. In this part, we follow the same training pipeline but with the open source software of [Upkie](https://hackaday.io/project/185729-upkie-wheeled-biped-robots)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634ff93-f09f-4e0a-8d0f-547848f3900b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1189580/170496331-e1293dd3-b50c-40ee-9c2e-f75f3096ebd8.png\" style=\"height: 100px\" align=\"right\" />\n",
    "\n",
    "First, make sure you have a C++ compiler (setup one-liners: [Fedora](https://github.com/upkie/upkie/discussions/100), [Ubuntu](https://github.com/upkie/upkie/discussions/101)). You can run an Upkie simulation right from the command line. It won't install anything on your machine, everything will run locally from the repository:\n",
    "\n",
    "```console\n",
    "git clone https://github.com/upkie/upkie.git\n",
    "cd upkie\n",
    "git checkout fb9a0ab1f67a8014c08b34d7c0d317c7a8f71662\n",
    "./start_simulation.sh\n",
    "```\n",
    "\n",
    "**NB:** this tutorial is written for the specific commit checked out above. If some instructions don't work it's likely you forgot to check it out.\n",
    "\n",
    "We will use the Python API of the robot to test things from this notebook, or from custom scripts. Install it from PyPI in your Conda environment:\n",
    "\n",
    "```\n",
    "pip install upkie\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44abc0-f7e9-4c2b-9d4e-a3579213e138",
   "metadata": {},
   "source": [
    "## Stepping the environment\n",
    "\n",
    "If everything worked well, you should be able to step an environment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "acedf0d6-fc2f-43f4-9ff6-a8e12dbd7ae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'upkie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mupkie\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m upkie\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39mregister()\n\u001b[1;32m      6\u001b[0m episode_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'upkie'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import upkie.envs\n",
    "\n",
    "upkie.envs.register()\n",
    "\n",
    "episode_return = 0.0\n",
    "with gym.make(\"UpkieGroundVelocity-v1\", frequency=200.0) as env:\n",
    "    observation, _ = env.reset()  # connects to the spine (simulator or real robot)\n",
    "    action = 0.0 * env.action_space.sample()\n",
    "    for step in range(1000):\n",
    "        pitch = observation[0]\n",
    "        action[0] = 10.0 * pitch  # 1D action: [ground_velocity]\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_return += reward\n",
    "        if terminated or truncated:\n",
    "            observation, _ = env.reset()\n",
    "\n",
    "print(f\"We have stepped the environment {step + 1} times\")\n",
    "print(f\"The return of our episode is {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031343b5-cf94-46ae-98f3-a4c5ebbc037c",
   "metadata": {},
   "source": [
    "(If you see a message \"Waiting for spine /vulp to start\", it means the simulation is not running.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecfd91f-676c-4d6f-beb0-a286dc681ae3",
   "metadata": {},
   "source": [
    "We can double-check the last observation from the episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d972f-4cc9-4005-b9a1-4a7433a19938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_last_observation(observation):\n",
    "    print(\"The last observation of the episode is:\")\n",
    "    print(f\"- Pitch from torso to world: {observation[0]:.2} rad\")\n",
    "    print(f\"- Ground position: {observation[1]:.2} m\")\n",
    "    print(f\"- Angular velocity from torso to world in torso: {observation[2]:.2} rad/s\")\n",
    "    print(f\"- Ground velocity: {observation[3]:.2} m/s\")\n",
    "    \n",
    "report_last_observation(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a269e3-d876-4d05-88e5-b0d73be6f939",
   "metadata": {},
   "source": [
    "## Question B1: PID control\n",
    "\n",
    "Adapt your code from Question 1 to this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e9f1e-f2a1-4a18-aa38-256a425d018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_b1(observation):\n",
    "    return np.array([0.0])  # replace with your solution\n",
    "\n",
    "\n",
    "def run(policy, nb_steps: int):\n",
    "    episode_return = 0.0\n",
    "    with gym.make(\"UpkieGroundVelocity-v1\", frequency=200.0) as env:\n",
    "        observation, _ = env.reset()  # connects to the spine (simulator or real robot)\n",
    "        for step in range(nb_steps):\n",
    "            action = policy_b1(observation)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                print(\"Fall detected!\")\n",
    "                return episode_return\n",
    "    report_last_observation(observation)\n",
    "    return episode_return\n",
    "\n",
    "\n",
    "episode_return = run(policy_b1, 1000)\n",
    "print(f\"The return of our episode is {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999eb22-a94a-4a58-ac06-9a7dbc15a7ee",
   "metadata": {},
   "source": [
    "## Training a new policy\n",
    "\n",
    "The Upkie repository ships three agents based on PID control, model predictive control and reinforcement learning. We now focus on the latter, called the \"PPO balancer\".\n",
    "\n",
    "Check that you can run the training part by running, from the root of the repository:\n",
    "\n",
    "```\n",
    "./tools/bazel run //agents/ppo_balancer:train -- --nb-envs 1 --show\n",
    "```\n",
    "\n",
    "A simulation window should pop, and verbose output from SB3 should be printed to your terminal.\n",
    "\n",
    "By default, training data will be logged to `/tmp`. You can select a different output path by setting the `UPKIE_TRAINING_PATH` environment variable in your shell. For instance:\n",
    "\n",
    "```\n",
    "export UPKIE_TRAINING_PATH=\"${HOME}/src/upkie/training\"\n",
    "```\n",
    "\n",
    "Run TensorBoard from the training directory:\n",
    "\n",
    "```\n",
    "tensorboard --logdir ${UPKIE_TRAINING_PATH}  # or /tmp if you keep the default\n",
    "```\n",
    "\n",
    "Each training will be named after a word picked at random in an English dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e47aad-7787-409c-af7e-b83bfccaa592",
   "metadata": {},
   "source": [
    "## Selecting the number of processes\n",
    "\n",
    "We can increase the number of parallel CPU environments ``--nb-envs`` to a value suitable to your computer. Let training run for a minute and check `time/fps`. Increase the number of environments and compare the stationary regime of `time/fps`. You should see a performance increase when adding the first few environments, followed by a declined when there are two many parallel processes compared to your number of CPU cores. Pick the value that works best for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a0943-cc10-4cd0-a2d8-d5313dbe37e5",
   "metadata": {},
   "source": [
    "## Running a trained policy\n",
    "\n",
    "Copy the file `final.zip` from your trained policy directory to `agents/ppo_balancer/policy/params.zip`. Start a simulation and run the policy by:\n",
    "\n",
    "```\n",
    "./tools/bazel run //agents/ppo_balancer\n",
    "```\n",
    "\n",
    "What kind of behavior do you observe?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaabe73c-f412-44b5-a714-241077720d01",
   "metadata": {},
   "source": [
    "== Your observations here =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c356c81-b5ef-4364-a5db-c8e2600e104a",
   "metadata": {},
   "source": [
    "## Question B2: Improve this baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ecb8c-7292-432f-b0d3-b90c36de8719",
   "metadata": {},
   "source": [
    "The policy you are testing here is not the one we saw in class. Open question: improve on it using any of the methods we discussed. Measure the improvement by `ep_len_mean` or any other quantitative criterion:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce7d720b-17b8-493d-8128-e66c6571d3ff",
   "metadata": {},
   "source": [
    "== Your experiments here ==\n",
    "\n",
    "- Tried: ... / Measured outcome: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
